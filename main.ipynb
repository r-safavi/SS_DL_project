{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "from model import RecEnvSimModel\n",
    "from data_loader import PongDataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image shape is  (64, 80)\n"
     ]
    }
   ],
   "source": [
    "train_data_loader = PongDataLoader('dataset40.zip')\n",
    "test_data_loader = PongDataLoader('dataset10.zip')\n",
    "\n",
    "# get shapes\n",
    "dim1 , dim2 = train_data_loader.first_dim , train_data_loader.second_dim\n",
    "print('image shape is ',(dim1,dim2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "LSTM_config = (32*2*3,\n",
    "               100,\n",
    "               3,\n",
    "               200)\n",
    "\n",
    "encoder_config = (1, # in_channels\n",
    "                  [64,32,32,32], # num_filters_list \n",
    "                  [(8,8),(6,6),(6,6),(4,4)], # filter_size_list\n",
    "                  [2 for _ in range(4)], # stride_list\n",
    "                  [(1,1),(1,1),(1,1),(0,0)]) # padding_list\n",
    "\n",
    "decoder_config = (100, # input_length\n",
    "                  [32,2,3], # deconv_input_shape\n",
    "                  [32,32,64,1], # num_filters_list \n",
    "                  [(4,4),(6,6),(6,6),(8,8)], # filter_size_list\n",
    "                  [2 for _ in range(4)], # stride_list\n",
    "                  [(0,0),(1,1),(1,1),(1,1)]) # padding_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RecEnvSimModel(LSTM_config, encoder_config, decoder_config)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rsafa\\Desktop\\Ramin\\venv\\lib\\site-packages\\torch\\nn\\functional.py:1967: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "c:\\Users\\rsafa\\Desktop\\Ramin\\venv\\lib\\site-packages\\torch\\nn\\functional.py:1956: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    }
   ],
   "source": [
    "temp_states, temp_actions = train_data_loader.get_data(1,15)\n",
    "states_tensor = torch.Tensor(temp_states) # of shape (1,16,dim1,dim2)\n",
    "actions_tensor = nn.functional.one_hot(torch.Tensor(temp_actions.astype(int)).long(),num_classes=3) # if I don't put the long() method, one_hot won't work\n",
    "# actions_tensor = torch.transpose(actions_tensor,-1,-2).float() # swap dimensions -1 and -2\n",
    "actions_tensor = actions_tensor[:,:,:,None].float()\n",
    "\n",
    "# actions_tensor is of shape (1,16,3)\n",
    "init_hidden = torch.zeros([1,100])\n",
    "init_cell = torch.zeros([1,100])\n",
    "out, hc = model(states_tensor[0:1,0:1,:,:], actions_tensor[0:1,0,:,:], (init_hidden,init_cell))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 100])\n",
      "torch.Size([1, 100])\n",
      "torch.Size([1, 1, 64, 80])\n",
      "torch.Size([1, 1, 64, 80])\n"
     ]
    }
   ],
   "source": [
    "print(hc[0].shape)\n",
    "print(hc[1].shape)\n",
    "print(states_tensor[0:1,0:1,:,:].shape)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train step 0\n",
      "Train step 1\n",
      "Train step 2\n",
      "Train step 3\n",
      "Train step 4\n",
      "Train step 5\n",
      "Train step 6\n",
      "Train step 7\n",
      "Train step 8\n",
      "Train step 9\n",
      "Train step 10\n",
      "Train step 11\n",
      "Train step 12\n",
      "Train step 13\n",
      "Train step 14\n",
      "Train step 15\n",
      "Train step 16\n",
      "Train step 17\n",
      "Train step 18\n",
      "Train step 19\n",
      "Train step 20\n",
      "Train step 21\n",
      "Train step 22\n",
      "Train step 23\n",
      "Train step 24\n",
      "Train step 25\n",
      "Train step 26\n",
      "Train step 27\n",
      "Train step 28\n",
      "Train step 29\n",
      "Train step 30\n",
      "Train step 31\n",
      "Train step 32\n",
      "Train step 33\n",
      "Train step 34\n",
      "Train step 35\n",
      "Train step 36\n",
      "Train step 37\n",
      "Train step 38\n",
      "Train step 39\n",
      "Train step 40\n",
      "Train step 41\n",
      "Train step 42\n",
      "Train step 43\n",
      "Train step 44\n",
      "Train step 45\n",
      "Train step 46\n",
      "Train step 47\n",
      "Train step 48\n",
      "Train step 49\n",
      "Train step 50\n",
      "Train step 51\n",
      "Train step 52\n",
      "Train step 53\n",
      "Train step 54\n",
      "Train step 55\n"
     ]
    }
   ],
   "source": [
    "n_training_steps = 100\n",
    "SEQ_LENGTH = 15\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "for train_step in range(n_training_steps):\n",
    "    if train_step%1==0: print('Train step',train_step)\n",
    "    train_loss = 0.0\n",
    "    states, actions = train_data_loader.get_data(BATCH_SIZE,SEQ_LENGTH)\n",
    "    states_tensor = torch.Tensor(states) # of shape (BATCH_SIZE,SEQ_LENGTH+1,dim1,dim2)\n",
    "    actions_tensor = nn.functional.one_hot(torch.Tensor(actions.astype(int)).long(),num_classes=3) # if I don't put the long() method, one_hot won't work\n",
    "    actions_tensor = actions_tensor[:,:,:,None].float()\n",
    "    output_tensor = torch.zeros((BATCH_SIZE,SEQ_LENGTH,dim1,dim2))\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    h = torch.zeros([1,100])\n",
    "    c = torch.zeros([1,100])\n",
    "    for i in range(SEQ_LENGTH):\n",
    "        out,(h,c) = model(states_tensor[:,i:i+1,:,:],\n",
    "                          actions_tensor[:,i,:,:],\n",
    "                          (h,c))\n",
    "        output_tensor[:,i:i+1,:,:] = out\n",
    "        pass\n",
    "    loss = criterion(output_tensor,states_tensor[:,1:,:,:])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16, 16)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
